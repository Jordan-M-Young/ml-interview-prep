# Questions

## 1. What is a transformer?

## 2. What is an encoder?

## 3. What is a decoder?

## 4. What is attention?

## 5. What is cross attention?

## 6. What is self attention?

## 7. What is the basic attention formula?

## 8. What is the time complexity of Attention?

## 9. Explain how Transformers improve on LSTMs and RNNs

## 10. What is Multi-head attention?

## 11. What are positional encodings for?

## 12. How do Transformers allow for parallel token processing?